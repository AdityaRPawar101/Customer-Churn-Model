{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Imports the NumPy library for numerical operations and array handling.\n",
    "import pandas as pd  # Imports the pandas library for data manipulation and analysis.\n",
    "import matplotlib.pyplot as plt  # Imports Matplotlib's pyplot for creating static visualizations.\n",
    "import seaborn as sns  # Imports Seaborn for statistical data visualization, built on top of Matplotlib.\n",
    "import plotly.express as px  # Imports Plotly Express for easy-to-use interactive visualizations.\n",
    "import missingno as msno  # Imports Missingno for visualizing missing data.\n",
    "from sklearn.pipeline import Pipeline  # Imports Pipeline for creating machine learning workflows.\n",
    "from sklearn.linear_model import LogisticRegression  # Imports LogisticRegression for classification tasks.\n",
    "from sklearn.ensemble import RandomForestClassifier  # Imports RandomForestClassifier for ensemble-based classification.\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay  # Imports metrics for evaluating model performance.\n",
    "from sklearn.model_selection import train_test_split  # Imports function to split data into training and testing sets.\n",
    "from sklearn.experimental import enable_iterative_imputer  # Enables the experimental IterativeImputer in scikit-learn.\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer  # Imports imputers to handle missing data.\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder  # Imports tools for data preprocessing (scaling and encoding).\n",
    "from sklearn.compose import ColumnTransformer  # Imports ColumnTransformer for applying different preprocessing steps to different columns.\n",
    "from sklearn.model_selection import GridSearchCV  # Imports GridSearchCV for hyperparameter tuning.\n",
    "from xgboost import XGBClassifier  # Imports XGBClassifier for gradient boosting-based classification.\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suppresses warnings to keep the output clean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and Explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"D:\\Programs\\Churn Prediction Model Experiment\\data\\E_Commerce_Dataset.xlsx\", sheet_name=\"E Comm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=\"CustomerID\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change column names to lowercase\n",
    "df.columns = [col.lower() for col in df.columns]\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segment the columns of the DataFrame into two lists based on the number of unique values each column contains.\n",
    "count_col = []\n",
    "hist_col = []\n",
    "for column in df.columns:\n",
    "    unique_value = df[column].nunique()\n",
    "    if unique_value <= 20:\n",
    "        count_col.append(column)\n",
    "    else:\n",
    "        hist_col.append(column)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Countplots of categorical columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,40))\n",
    "plot_num = 1\n",
    "for col in count_col:\n",
    "    plt.subplot(10,2,plot_num)\n",
    "    sns.countplot(data=df, x=col)\n",
    "    plot_num += 1\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Histograms of numerical columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,40))\n",
    "plot_num = 1\n",
    "for col in hist_col:\n",
    "    plt.subplot(10,2,plot_num)\n",
    "    sns.histplot(data=df, x=col,bins=25)\n",
    "    plot_num += 1\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the Churn distribution for each categorical variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,40))\n",
    "plot_num = 1\n",
    "for col in count_col:\n",
    "    if df[col].nunique() <= 8 and col != \"churn\":\n",
    "        plt.subplot(10,2,plot_num)\n",
    "        sns.countplot(data=df, x=col, hue=\"churn\")\n",
    "        plot_num += 1\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer  # Imports SimpleImputer for handling missing data with basic strategies.\n",
    "from sklearn.experimental import enable_iterative_imputer  # Enables the experimental IterativeImputer in scikit-learn.\n",
    "from sklearn.impute import IterativeImputer  # Imports IterativeImputer for advanced imputation techniques using iterative models.\n",
    "from sklearn.ensemble import RandomForestRegressor  # Imports RandomForestRegressor for regression tasks using ensemble methods.\n",
    "import pandas as pd  # Imports the pandas library for data manipulation and analysis.\n",
    "\n",
    "def fill_missing_values(df, random_state=None):\n",
    "    # Step 1: Identify numeric and categorical columns\n",
    "    numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns.tolist()  # Include both string and category data\n",
    "\n",
    "    # Step 2: Impute numeric columns\n",
    "    numeric_imputer = SimpleImputer(strategy='mean')\n",
    "    df[numeric_columns] = numeric_imputer.fit_transform(df[numeric_columns])\n",
    "\n",
    "    # Step 3: Handle categorical columns\n",
    "    for col in categorical_columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            # Convert categorical column to one-hot encoded representation\n",
    "            encoded_cols = pd.get_dummies(df[col], prefix=col)\n",
    "            # Concatenate one-hot encoded columns\n",
    "            df = pd.concat([df.drop(col, axis=1), encoded_cols], axis=1)\n",
    "\n",
    "    # Step 4: Random Forest Iterative Imputer for the entire DataFrame\n",
    "    rf_imputer = IterativeImputer(estimator=RandomForestRegressor(random_state=random_state))\n",
    "    df = pd.DataFrame(rf_imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Call the function to fill missing values\n",
    "df = fill_missing_values(df, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Dataset into Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split model into training and test set\n",
    "X = df.drop(columns=[\"churn\"])\n",
    "y = df[\"churn\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling the Imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "print('Before upsampling count of label 0 {}'.format(sum(y_train==0)))\n",
    "print('Before upsampling count of label 1 {}'.format(sum(y_train==1)))\n",
    "# Minority Over Sampling Technique\n",
    "sm = SMOTE(sampling_strategy = 1, random_state=1)   \n",
    "X_train_s, y_train_s = sm.fit_resample(X_train, y_train.ravel())\n",
    "                                         \n",
    "print('After upsampling count of label 0 {}'.format(sum(y_train_s==0)))\n",
    "print('After upsampling count of label 1 {}'.format(sum(y_train_s==1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Evaluation with Cross Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV, RidgeClassifierCV, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier, RandomForestClassifier \n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the models\n",
    "models=[\n",
    "    #Ensemble\n",
    "    AdaBoostClassifier(),\n",
    "    BaggingClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    \n",
    "    #Linear Models\n",
    "    LogisticRegressionCV(),\n",
    "    RidgeClassifierCV(),\n",
    "    \n",
    "    #Nearest Neighbour\n",
    "    KNeighborsClassifier(),\n",
    "    \n",
    "    #XGBoost\n",
    "    XGBClassifier()\n",
    "]\n",
    "\n",
    "metrics_cols = ['model_name','test_accuracy','test_precision','test_recall','test_f1']\n",
    "\n",
    "model_name=[]\n",
    "test_acuracy=[]\n",
    "test_precision=[]\n",
    "test_recall=[]\n",
    "test_f1=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "\n",
    "scoring = ['accuracy','precision', 'recall', 'f1']\n",
    "\n",
    "for model in models:\n",
    "    cv_results = model_selection.cross_validate(model, X, y, cv=5, \n",
    "                                                scoring=scoring, return_train_score=True)\n",
    "    model_name.append(model.__class__.__name__)\n",
    "    test_acuracy.append(round(cv_results['test_accuracy'].mean(),3)*100)\n",
    "    test_precision.append(round(cv_results['test_precision'].mean(),3)*100)\n",
    "    test_recall.append(round(cv_results['test_recall'].mean(),3)*100)\n",
    "    test_f1.append(round(cv_results['test_f1'].mean(),3)*100)\n",
    "\n",
    "metrics_data = [model_name, test_acuracy, test_precision, test_recall, test_f1]\n",
    "m = {n:m for n,m in zip(metrics_cols,metrics_data)}\n",
    "model_metrics = pd.DataFrame(m)\n",
    "model_metrics = model_metrics.sort_values('test_accuracy', ascending=False)\n",
    "metrics_styled = model_metrics.style.background_gradient(subset=['test_accuracy', 'test_f1'], cmap='summer')\n",
    "metrics_styled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model=RandomForestClassifier()\n",
    "final_model.fit(X_train, y_train)\n",
    "train_pred = final_model.predict(X_train)\n",
    "test_pred = final_model.predict(X_test)\n",
    "\n",
    "final_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model=XGBClassifier()\n",
    "final_model.fit(X_train, y_train)\n",
    "train_pred = final_model.predict(X_train)\n",
    "test_pred = final_model.predict(X_test)\n",
    "\n",
    "final_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Importance of XGBoost Model**\n",
    "- Importance_type\n",
    "    - *‘weight’* - the number of times a feature is used to split the data across all trees.\n",
    "    - *‘gain’* - the average gain across all splits the feature is used in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature Importance Gain\")\n",
    "feature_important = final_model.get_booster().get_score(importance_type=\"gain\")\n",
    "keys = list(feature_important.keys())\n",
    "values = list(feature_important.values())\n",
    "\n",
    "data = pd.DataFrame(data=values, index=keys, columns=[\"score\"]).sort_values(by = \"score\", ascending=False)\n",
    "data.nlargest(20, columns=\"score\").plot(kind='barh', figsize = (20,10)) ## plot top 40 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature Importance Weight\")\n",
    "feature_important = final_model.get_booster().get_score(importance_type='weight')\n",
    "keys = list(feature_important.keys())\n",
    "values = list(feature_important.values())\n",
    "\n",
    "data = pd.DataFrame(data=values, index=keys, columns=[\"score\"]).sort_values(by = \"score\", ascending=False)\n",
    "data.nlargest(20, columns=\"score\").plot(kind='barh', figsize = (20,10)) ## plot top 40 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Simpler models with less but more important features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The features that will be used for final deployment model are:\n",
    "    - Tenure\n",
    "    - Cashback amount\n",
    "    - City tier\n",
    "    - Warehouse to home\n",
    "    - Order amount hike from last year\n",
    "    - Days since last order\n",
    "    - Satisfaction score\n",
    "    - Number of address\n",
    "    - Number ofdevice registered\n",
    "    - Complain\n",
    "    - Order count\n",
    "    - hourspendonapp\n",
    "    - Marital status\n",
    "    - Coupon used\n",
    "    - Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the columns\n",
    "cols_to_drop = ['preferredlogindevice_Computer', 'preferredlogindevice_Mobile Phone', 'preferredlogindevice_Phone', \n",
    "                'preferredpaymentmode_CC', 'preferredpaymentmode_COD', 'preferredpaymentmode_Cash on Delivery', 'preferredpaymentmode_Credit Card', \n",
    "                'preferredpaymentmode_Debit Card', 'preferredpaymentmode_E wallet', 'preferredpaymentmode_UPI', 'preferedordercat_Fashion', \n",
    "                'preferedordercat_Grocery', 'preferedordercat_Laptop & Accessory', 'preferedordercat_Mobile', 'preferedordercat_Mobile Phone', 'preferedordercat_Others' ]\n",
    "X.drop(cols_to_drop, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(cols_to_drop, axis = 1, inplace = True)\n",
    "X_test.drop(cols_to_drop, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Pandas datafram to NumPy array for the XGBoost classifier. \n",
    "# The conversion helps in loading and predicting values in the flask app.\n",
    "\n",
    "X_test = X_test.values\n",
    "X_train = X_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "final_model=XGBClassifier()\n",
    "final_model.fit(X_train, y_train)\n",
    "train_pred = final_model.predict(X_train)\n",
    "test_pred = final_model.predict(X_test)\n",
    "\n",
    "final_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, test_pred, labels=final_model.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=final_model.classes_)\n",
    "disp.plot()\n",
    "plt.title('Confusion Matrix of Random Forest Classifier')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = final_model.predict(X_test)\n",
    "\n",
    "# Create a list to store the actual and predicted values\n",
    "results = []\n",
    "\n",
    "# Store the actual and predicted log_price values in the list\n",
    "for actual, predicted in zip(y_test, y_pred):\n",
    "    results.append((actual, predicted))\n",
    "\n",
    "# Convert the list into a DataFrame\n",
    "XGBoost_df = pd.DataFrame(results, columns=['Actual', 'Predicted'])\n",
    "\n",
    "# Print the DataFrame\n",
    "XGBoost_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(final_model,open('end_to_end_deployment/models/churn_prediction_model.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the data columns\n",
    "import json\n",
    "\n",
    "columns = {'data_columns' : [col.lower() for col in X.columns]}\n",
    "\n",
    "with open(\"end_to_end_deployment/models/columns.json\",\"w\") as f:\n",
    "    f.write(json.dumps(columns))    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
